{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# import cv2\n",
    "from skimage import color, io\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [x] convert dataset to tensor? (now: numpy array)\n",
    " - seems like dataloader does it automatically - but doesnt swap axes\n",
    "- [ ] normialize data?\n",
    "- [ ] transform images?\n",
    "- [ ] on deploy, increase num_workers in dataloader (>1)\n",
    "- [ ] deal with `torch.set_default_tensor_type('torch.DoubleTensor')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images and convert them to [*CIE Lab*](https://en.m.wikipedia.org/wiki/CIELAB_color_space) color space\n",
    "\n",
    "Warrning: `OpenCV` [uses](https://stackoverflow.com/questions/39316447/opencv-giving-wrong-color-to-colored-images-on-loading) BGR scheme, whereas `matplotlib` uses RGB. `scikit-image` uses RGB as well.\n",
    "\n",
    "\n",
    "[Scikit color ranges: L: 0 to 100, a: -127 to 128, b: -128 to 127.](https://stackoverflow.com/questions/25294141/cielab-color-range-for-scikit-image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dateset\n",
    "\n",
    "Notes\n",
    "\n",
    "One needs to [swap axes](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms)\n",
    "- numpy image: H x W x C\n",
    "- torch image: C X H X W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDateset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir):\n",
    "        \"\"\"\n",
    "        All images from `img_dir` will be read.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.img_names = [file for file in os.listdir(self.img_dir)]\n",
    "        \n",
    "        assert all([img.endswith('.jpg') for img in self.img_names]), \"Must be *.jpg\"\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an image in Lab color space.\n",
    "        Returns a tuple (L, ab)\n",
    "            - `L` stands for lightness - it's the net input\n",
    "            - `ab` is chrominance - something that the net learns\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        img_name = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        assert image.shape == (224, 224, 3)\n",
    "                \n",
    "        img_lab = color.rgb2lab(image)\n",
    "        \n",
    "        tsfm2tensor = transforms.ToTensor()\n",
    "        \n",
    "        img_lab = tsfm2tensor(img_lab)\n",
    "        \n",
    "        img_lab = img_lab.double()\n",
    "        \n",
    "\n",
    "        assert img_lab.shape == (3, 224, 224)\n",
    "               \n",
    "        L  = img_lab[:1,:,:]\n",
    "        ab = img_lab[1:,:,:]\n",
    "              \n",
    "        assert L.shape == (1, 224, 224)\n",
    "        assert ab.shape == (2, 224, 224)\n",
    "        \n",
    "        return (L, ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir='./data/rawcropped224/'\n",
    "\n",
    "trainset = ImagesDateset(img_dir)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 'Low-level features'\n",
    "        # conv1 has only one in channel - because it's only L channel of a photo\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 'Mid-level fetures'\n",
    "        self.conv7 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 'Colorization network'\n",
    "        self.conv9 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        \n",
    "        # UPSAMPLE 1\n",
    "        # Till now the picture should be H/8 x W/8\n",
    "        # since we fix H=W=224, then H/8=28. We want upsample to 56x56\n",
    "        # DEPRECATED - see forward()\n",
    "#         self.upsample1 = nn.Upsample(size=(256, 56, 56), mode='nearest')\n",
    "        \n",
    "        \n",
    "        self.conv10 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv11 = nn.Conv2d(in_channels=64,  out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # UPSAMPLE 2   -   We got 56x56, want: 112x112\n",
    "        # DEPRECATED - see forward()\n",
    "#         self.upsample2 = nn.Upsample(size=(64, 112, 112), mode='nearest')\n",
    "        \n",
    "        \n",
    "        self.conv12 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # ??????????????\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Low level\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        \n",
    "        \n",
    "        # Mid level\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        \n",
    "        assert x.shape == (4, 256, 28, 28), \"おわり： mid level\"\n",
    "        \n",
    "\n",
    "        # Colorization Net\n",
    "        x = F.relu(self.conv9(x))\n",
    "        \n",
    "        assert x.shape == (4, 128, 28, 28),　\"おわり： conv9\"\n",
    "        \n",
    "        \n",
    "#         x = self.upsample1(x)\n",
    "        x = nn.functional.interpolate(input=x, scale_factor=2, mode='nearest')\n",
    "    \n",
    "        assert x.shape == (4, 128, 56, 56), \"おわり： upsample1\"\n",
    "    \n",
    "        print(x.shape)\n",
    "        \n",
    "    \n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = F.relu(self.conv11(x))\n",
    "        \n",
    "#         x = self.upsample2(x)\n",
    "        x = nn.functional.interpolate(input=x, scale_factor=2, mode='nearest')\n",
    "\n",
    "\n",
    "        x = F.relu(self.conv12(x))\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L.shape\n",
      "torch.Size([4, 1, 224, 224])\n",
      "\n",
      "---\n",
      "\n",
      "Mid おわり。x.shape: torch.Size([4, 256, 28, 28])\n",
      "torch.Size([4, 128, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "dataiter = iter(trainloader)\n",
    "\n",
    "L, ab = dataiter.next()\n",
    "\n",
    "print(\"L.shape\")\n",
    "print(L.shape)\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "out = net(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 112, 112])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
