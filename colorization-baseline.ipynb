{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Image Colorization\n",
    "\n",
    "Based on [Let there be Color!](http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from skimage import color, io\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "img_dir_train ='./data/caprese_salad/'\n",
    "img_dir_test = './data/caprese_salad/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [x] convert dataset to tensor? (now: numpy array)\n",
    " - seems like dataloader does it automatically - but doesnt swap axes\n",
    " - transforms.toTensor does the job\n",
    "- [x] normialize data?\n",
    " - only $ab$ channel + L channel\n",
    "- [ ] transform images? - random crops itp\n",
    "- [ ] deal with `torch.set_default_tensor_type('torch.DoubleTensor')` or `torch.FloatTensor`\n",
    "- [x] refector asserts so they work with batch sizes different than 4\n",
    "- [ ] shuffle data (trainloader)\n",
    "- [ ] refactor?? - separate files\n",
    "- [x] load / save model\n",
    "- [x] extract hyperparameters\n",
    "- [x] load whole dataset to memory\n",
    "- [ ] read/write to zip\n",
    "- [ ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dateset\n",
    "\n",
    "- One needs to [swap axes](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms)\n",
    "    - numpy image: H x W x C\n",
    "    - torch image: C X H X W\n",
    "\n",
    "\n",
    "- Quick fact: $L \\in [0, 100]$, $a \\in [-127, 128]$, $b \\in [-128, 127]$ [(source)](https://stackoverflow.com/questions/25294141/cielab-color-range-for-scikit-image)\n",
    "\n",
    "\n",
    "- How an image is processed:\n",
    "    1. Load i-th image to memory\n",
    "    2. Convert it to LAB Space\n",
    "    3. Convert to torch.tensor\n",
    "    4. Split the image to $L$ and $ab$ channels\n",
    "    5. Normalize $ab$ to $[0, 1]$. $L$ ~~remains unnormalized.~~ is normalized as well.\n",
    "    6. $L$ will feed net, $a'b'$ will be its output\n",
    "    7. Calculate $Loss(ab, a'b')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDateset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, all2mem=True):\n",
    "        \"\"\"\n",
    "        All images from `img_dir` will be read.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.all2mem = all2mem\n",
    "        self.img_names = [file for file in os.listdir(self.img_dir)]\n",
    "        \n",
    "        assert all([img.endswith('.jpg') for img in self.img_names]), \"Must be *.jpg\"\n",
    "        \n",
    "        if self.all2mem:\n",
    "            self.images = [io.imread(os.path.join(self.img_dir, img)) \n",
    "                           for img in self.img_names]\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "   \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an image in Lab color space.\n",
    "        Returns a tuple (L, ab, name)\n",
    "            - `L` stands for lightness - it's the net input\n",
    "            - `ab` is chrominance - something that the net learns\n",
    "            - `name` - image filename\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.all2mem:\n",
    "            image = self.images[idx]\n",
    "        else:\n",
    "            img_name = os.path.join(self.img_dir, self.img_names[idx])\n",
    "            image = io.imread(img_name)\n",
    "        \n",
    "        \n",
    "        assert image.shape == (224, 224, 3)\n",
    "                \n",
    "        img_lab = color.rgb2lab(image)\n",
    "        img_lab = np.transpose(img_lab, (2, 0, 1))\n",
    "        \n",
    "        assert img_lab.shape == (3, 224, 224)\n",
    "        \n",
    "        img_lab = torch.tensor( img_lab.astype(np.float32) )\n",
    "        \n",
    "        assert img_lab.shape == (3, 224, 224)\n",
    "               \n",
    "        L  = img_lab[:1,:,:]\n",
    "        ab = img_lab[1:,:,:]\n",
    "        \n",
    "        # Normalization\n",
    "        L =   L / 100.0         # 0..1\n",
    "        ab = (ab + 128.0) / 255.0 # 0..1\n",
    "              \n",
    "        assert L.shape == (1, 224, 224)\n",
    "        assert ab.shape == (2, 224, 224)\n",
    "        \n",
    "        return L, ab, self.img_names[idx]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ImagesDateset(img_dir_train, all2mem=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "\n",
    "testset = ImagesDateset(img_dir_test, all2mem=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColNet(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ColNet, self).__init__()\n",
    "        \n",
    "        ksize = np.array( [1, 64, 128, 128, 256, 256, 512, 512, 256, 128, 64, 64, 32] ) // 8\n",
    "        ksize[0] = 1\n",
    "        \n",
    "        # 'Low-level features'\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,        out_channels=ksize[1], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=ksize[1], out_channels=ksize[2], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=ksize[2], out_channels=ksize[3], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=ksize[3], out_channels=ksize[4], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=ksize[4], out_channels=ksize[5], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=ksize[5], out_channels=ksize[6], kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 'Mid-level fetures'\n",
    "        self.conv7 = nn.Conv2d(in_channels=ksize[6], out_channels=ksize[7], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=ksize[7], out_channels=ksize[8], kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 'Colorization network'\n",
    "        self.conv9 = nn.Conv2d(in_channels=ksize[8], out_channels=ksize[9], kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Here comes upsample #1\n",
    "        \n",
    "        self.conv10 = nn.Conv2d(in_channels=ksize[9], out_channels=ksize[10], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv11 = nn.Conv2d(in_channels=ksize[10],out_channels=ksize[11], kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Here comes upsample #2        \n",
    "        \n",
    "        self.conv12 = nn.Conv2d(in_channels=ksize[11], out_channels=ksize[12], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv13out = nn.Conv2d(in_channels=ksize[12], out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Low level\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        \n",
    "        \n",
    "        # Mid level\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        \n",
    "        # assert x.shape[1:] == (256, 28, 28), \"おわり： mid level\"\n",
    "        \n",
    "\n",
    "        # Colorization Net\n",
    "        x = F.relu(self.conv9(x))\n",
    "        \n",
    "        # assert x.shape[1:] == (128, 28, 28), \"おわり： conv9\"\n",
    "        \n",
    "        x = nn.functional.interpolate(input=x, scale_factor=2, mode='nearest')\n",
    "\n",
    "        # assert x.shape[1:] == (128, 56, 56), \"おわり： upsample1\"\n",
    "    \n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = F.relu(self.conv11(x))\n",
    "        \n",
    "        x = nn.functional.interpolate(input=x, scale_factor=2, mode='nearest')\n",
    "\n",
    "\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = torch.sigmoid(self.conv13out(x))\n",
    "        \n",
    "        x = nn.functional.interpolate(input=x, scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # assert x.shape[1:] == (2, 224, 224)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "Unnormalize and return RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_out2rgb(L, ab_out):\n",
    "    \"\"\"\n",
    "    L - original `L` channel\n",
    "    ab_out - learned `ab` channels which were the net's output\n",
    "    \n",
    "    Retruns: 3 channel RGB image\n",
    "    \"\"\"\n",
    "    # Convert to numpy and unnnormalize\n",
    "    L = L.numpy() * 100.0\n",
    "    \n",
    "    ab_out = np.floor(ab_out.numpy() * 255.0) - 128.0 \n",
    "    \n",
    "    # Transpose axis to HxWxC again\n",
    "    L = L.transpose((1, 2, 0))\n",
    "    ab_out = ab_out.transpose((1, 2, 0))\n",
    "\n",
    "    # Stack layers\n",
    "    lab_stack = np.dstack((L, ab_out))\n",
    "    \n",
    "    return color.lab2rgb(lab_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ColNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "model_filename = \"\"\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        L, ab, _ = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ab_outputs = net(L)\n",
    "        \n",
    "        loss = criterion(ab, ab_outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        print('[epoch: {} | batch: {}] loss: {:.3f}'\n",
    "              .format(epoch + 1, i + 1, running_loss / BATCH_SIZE))\n",
    "    \n",
    "        epoch_loss += running_loss\n",
    "    \n",
    "    print('mean epoch loss: {:.2f}'.format(epoch_loss / (BATCH_SIZE * len(trainloader))))\n",
    "        \n",
    "    # Save\n",
    "    # model_filename = './model/colnet{}.pt'.format(time.strftime(\"%y%m%d-%H-%M-%S\"))\n",
    "    model_filename = './model/colnet.pt'\n",
    "    torch.save(net.state_dict(), model_filename)\n",
    "    print('saved model to {}'.format(model_filename))\n",
    "    \n",
    "    print('End of epoch {}\\n'.format(epoch + 1))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"./model/colnet.pt\"\n",
    "\n",
    "print(\"Make sure you're using up to date model!!!\")    \n",
    "\n",
    "net = ColNet()\n",
    "net.load_state_dict(torch.load(model_filename))\n",
    "net.eval()\n",
    "\n",
    "print(\"Colorizing {} using {}\\n\".format(img_dir_test, model_filename))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_no, data in enumerate(testloader):\n",
    "        \n",
    "        print(\"Processing batch {} / {}\".format(batch_no + 1, len(testloader)))\n",
    "        L, ab, name = data\n",
    "        ab_outputs = net(L)\n",
    "        \n",
    "        for i in range(L.shape[0]):\n",
    "            img = net_out2rgb(L[i], ab_outputs[i])\n",
    "            io.imsave(os.path.join(\"./out/\", name[i]), img)\n",
    "\n",
    "print(\"Saved all photos to ./out/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
