{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from skimage import color, io\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [x] convert dataset to tensor? (now: numpy array)\n",
    " - seems like dataloader does it automatically - but doesnt swap axes\n",
    " - transforms.toTensor does the job\n",
    "- [x] normialize data?\n",
    " - only $ab$ channel \n",
    "- [ ] transform images? - random crops itp\n",
    "- [ ] on deploy, increase num_workers in dataloader (>1)\n",
    "- [ ] deal with `torch.set_default_tensor_type('torch.DoubleTensor')`\n",
    "- [ ] should I use `torch.nn.functional.{relu | sigmoid}` or just `torch.nn.*`?\n",
    "- [x] refector asserts so they work with batch sizes different than 4\n",
    "- [ ] shuffle data (trainloader)\n",
    "- [ ] refactor?? - separate files\n",
    "- [ ] load / save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images and convert them to [*CIE Lab*](https://en.m.wikipedia.org/wiki/CIELAB_color_space) color space\n",
    "\n",
    "Warrning: `OpenCV` [uses](https://stackoverflow.com/questions/39316447/opencv-giving-wrong-color-to-colored-images-on-loading) BGR scheme, whereas `matplotlib` uses RGB. `scikit-image` uses RGB as well.\n",
    "\n",
    "\n",
    "[Scikit color ranges: L: 0 to 100, a: -127 to 128, b: -128 to 127.](https://stackoverflow.com/questions/25294141/cielab-color-range-for-scikit-image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dateset\n",
    "\n",
    "Notes\n",
    "\n",
    "One needs to [swap axes](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms)\n",
    "- numpy image: H x W x C\n",
    "- torch image: C X H X W\n",
    "\n",
    "\n",
    "Quick facts: $L \\in [0, 100]$, $a \\in [-127, 128]$, $b \\in [-128, 127]$\n",
    "\n",
    "How an image is processed:\n",
    "1. Load i-th image to memory\n",
    "2. Convert it to LAB Space\n",
    "3. Convert to torch.tensor\n",
    "4. Split the image to $L$ and $ab$ channels\n",
    "5. Normalize $ab$ to $[0, 1]$. $L$ remains unnormalized.\n",
    "6. $L$ will feed net, $a'b'$ will be its output\n",
    "7. Calculate $Loss(ab, a'b')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDateset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir):\n",
    "        \"\"\"\n",
    "        All images from `img_dir` will be read.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.img_names = [file for file in os.listdir(self.img_dir)]\n",
    "        \n",
    "        assert all([img.endswith('.jpg') for img in self.img_names]), \"Must be *.jpg\"\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an image in Lab color space.\n",
    "        Returns a tuple (L, ab)\n",
    "            - `L` stands for lightness - it's the net input\n",
    "            - `ab` is chrominance - something that the net learns\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        img_name = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        assert image.shape == (224, 224, 3)\n",
    "                \n",
    "        img_lab = color.rgb2lab(image)\n",
    "        \n",
    "        tsfm2tensor = transforms.ToTensor()\n",
    "        \n",
    "        img_lab = tsfm2tensor(img_lab)        \n",
    "        img_lab = img_lab.double()\n",
    "        \n",
    "        assert img_lab.shape == (3, 224, 224)\n",
    "               \n",
    "        L  = img_lab[:1,:,:]\n",
    "        ab = img_lab[1:,:,:]\n",
    "        \n",
    "        # Normalize to (0, 1)\n",
    "        ab = (ab + 128.0) / 256.0\n",
    "              \n",
    "        assert L.shape == (1, 224, 224)\n",
    "        assert ab.shape == (2, 224, 224)\n",
    "        \n",
    "        return (L, ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir_train ='./data/food-train/'\n",
    "img_dir_test = './data/food-test/'\n",
    "\n",
    "trainset = ImagesDateset(img_dir_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "\n",
    "testset = ImagesDateset(img_dir_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 'Low-level features'\n",
    "        # conv1 has only one in channel - because it's only L channel of a photo\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 'Mid-level fetures'\n",
    "        self.conv7 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 'Colorization network'\n",
    "        self.conv9 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Here comes upsample #1\n",
    "        \n",
    "        self.conv10 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv11 = nn.Conv2d(in_channels=64,  out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Here comes upsample #2        \n",
    "        \n",
    "        self.conv12 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.conv13out = nn.Conv2d(in_channels=32, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Low level\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        \n",
    "        \n",
    "        # Mid level\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        \n",
    "        assert x.shape[1:] == (256, 28, 28), \"おわり： mid level\"\n",
    "        \n",
    "\n",
    "        # Colorization Net\n",
    "        x = F.relu(self.conv9(x))\n",
    "        \n",
    "        assert x.shape[1:] == (128, 28, 28), \"おわり： conv9\"\n",
    "        \n",
    "        x = nn.functional.interpolate(input=x, scale_factor=2, mode='nearest')\n",
    "\n",
    "        assert x.shape[1:] == (128, 56, 56), \"おわり： upsample1\"\n",
    "    \n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = F.relu(self.conv11(x))\n",
    "        \n",
    "        x = nn.functional.interpolate(input=x, scale_factor=2, mode='nearest')\n",
    "\n",
    "\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = torch.sigmoid(self.conv13out(x))\n",
    "        \n",
    "        x = nn.functional.interpolate(input=x, scale_factor=2, mode='nearest')\n",
    "        \n",
    "        assert x.shape[1:] == (2, 224, 224)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_out2rgb(L, ab_out):\n",
    "    \"\"\"\n",
    "    L - original `L` channel\n",
    "    ab_out - learned `ab` channels which were the net's output\n",
    "    \n",
    "    Retruns: 3 channel RGB image\n",
    "    \"\"\"\n",
    "    # Convert to numpy and unnnormalize\n",
    "    L = L.numpy()\n",
    "    ab_out = ab_out.numpy() * 256.0 - 128.0\n",
    "    \n",
    "    # Transpose axis to HxWxC again\n",
    "    L = L.transpose((1, 2, 0))\n",
    "    ab_out = ab_out.transpose((1, 2, 0))\n",
    "\n",
    "    # Stack layers\n",
    "    lab_stack = np.dstack((L, ab_out))\n",
    "    \n",
    "    return color.lab2rgb(lab_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 0\n",
      "End of epoch 1\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(2): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        L, ab = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ab_outputs = net(L)\n",
    "        \n",
    "        loss = criterion(ab, ab_outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        #if i % 2000 == 1999:    \n",
    "#         print('[%d, %5d] loss: %.3f' %\n",
    "#               (epoch + 1, i + 1, running_loss / 2000))\n",
    "#         running_loss = 0.0\n",
    "\n",
    "    print('End of epoch {}'.format(epoch))\n",
    "    torch.save(net.state_dict(), './model/colnet' + time.strftime(\"%y%m%d-%H-%M-%S\") + \".pt\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save / Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './data/colnet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load('./data/colnet.pt'))\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "\n",
    "        L, ab = data\n",
    "        ab_outputs = net(L)\n",
    "        \n",
    "        for i in range(L.shape[0]):\n",
    "            print(i)\n",
    "            img = net_out2rgb(L[i], ab_outputs[i])\n",
    "            io.imsave(\"./out/{}.jpg\".format(i), img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
