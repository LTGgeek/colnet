{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Image Colorization\n",
    "\n",
    "Based on [Let there be Color!](http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from skimage import color, io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 30\n",
    "img_dir_train = './data/food41-120-train/'\n",
    "img_dir_test  = './data/food41-120-train/' #test/'\n",
    "img_dir_dev   = './data/food41-120-test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- [x] convert dataset to tensor? (now: numpy array)\n",
    " - seems like dataloader does it automatically - but doesnt swap axes\n",
    " - transforms.toTensor does the job\n",
    "- [x] normialize data?\n",
    " - only $ab$ channel + L channel\n",
    "- [ ] transform images? - random crops itp\n",
    "- [ ] deal with `torch.set_default_tensor_type('torch.DoubleTensor')` or `torch.FloatTensor`\n",
    "- [x] refector asserts so they work with batch sizes different than 4\n",
    "- [ ] shuffle data (trainloader)\n",
    "- [ ] refactor?? - separate files\n",
    "- [x] load / save model\n",
    "- [x] extract hyperparameters\n",
    "- [x] load whole dataset to memory\n",
    "- [x] read/write to zip\n",
    "- [ ] deal with `Color data out of range: Z < 0 in %s pixels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dateset\n",
    "\n",
    "- One needs to [swap axes](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms)\n",
    "    - numpy image: H x W x C\n",
    "    - torch image: C X H X W\n",
    "\n",
    "\n",
    "- Quick fact: $L \\in [0, 100]$, $a \\in [-127, 128]$, $b \\in [-128, 127]$ [(source)](https://stackoverflow.com/questions/25294141/cielab-color-range-for-scikit-image)\n",
    "\n",
    "\n",
    "- How an image is processed:\n",
    "    1. Load i-th image to memory\n",
    "    2. Convert it to LAB Space\n",
    "    3. Convert to torch.tensor\n",
    "    4. Split the image to $L$ and $ab$ channels\n",
    "    5. Normalize $ab$ to $[0, 1]$. $L$ ~~remains unnormalized.~~ is normalized as well.\n",
    "    6. $L$ will feed net, $a'b'$ will be its output\n",
    "    7. Calculate $Loss(ab, a'b')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDateset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir, all2mem=True):\n",
    "        \"\"\"\n",
    "        All images from `img_dir` will be read.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.all2mem = all2mem\n",
    "        self.img_names = [file for file in os.listdir(self.img_dir)]\n",
    "        \n",
    "        assert all([img.endswith('.jpg') for img in self.img_names]), \"Must be *.jpg\"\n",
    "        \n",
    "        if self.all2mem:\n",
    "            self.images = [io.imread(os.path.join(self.img_dir, img)) \n",
    "                           for img in self.img_names]\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "   \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an image in Lab color space.\n",
    "        Returns a tuple (L, ab, name)\n",
    "            - `L` stands for lightness - it's the net input\n",
    "            - `ab` is chrominance - something that the net learns\n",
    "            - `name` - image filename\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.all2mem:\n",
    "            image = self.images[idx]\n",
    "        else:\n",
    "            img_name = os.path.join(self.img_dir, self.img_names[idx])\n",
    "            image = io.imread(img_name)\n",
    "        \n",
    "        \n",
    "        assert image.shape == (224, 224, 3)\n",
    "                \n",
    "        img_lab = color.rgb2lab(image)\n",
    "        img_lab = np.transpose(img_lab, (2, 0, 1))\n",
    "        \n",
    "        assert img_lab.shape == (3, 224, 224)\n",
    "        \n",
    "        img_lab = torch.tensor( img_lab.astype(np.float32) )\n",
    "        \n",
    "        assert img_lab.shape == (3, 224, 224)\n",
    "               \n",
    "        L  = img_lab[:1,:,:]\n",
    "        ab = img_lab[1:,:,:]\n",
    "        \n",
    "        # Normalization\n",
    "        L =   L / 100.0         # 0..1\n",
    "        ab = (ab + 128.0) / 255.0 # 0..1\n",
    "              \n",
    "        assert L.shape == (1, 224, 224)\n",
    "        assert ab.shape == (2, 224, 224)\n",
    "        \n",
    "        return L, ab, self.img_names[idx]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ImagesDateset(img_dir_train, all2mem=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "\n",
    "testset = ImagesDateset(img_dir_test, all2mem=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "\n",
    "devset = ImagesDateset(img_dir_dev, all2mem=True)\n",
    "devloader = torch.utils.data.DataLoader(devset, batch_size=BATCH_SIZE,\n",
    "                                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColNet(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ColNet, self).__init__()\n",
    "        \n",
    "        ksize = np.array( [1, 64, 128, 128, 256, 256, 512, 512, 256, 128, 64, 64, 32] ) // 8\n",
    "        ksize[0] = 1\n",
    "        \n",
    "        # 'Low-level features'\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,        out_channels=ksize[1], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=ksize[1], out_channels=ksize[2], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=ksize[2], out_channels=ksize[3], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=ksize[3], out_channels=ksize[4], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=ksize[4], out_channels=ksize[5], kernel_size=3, stride=2, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=ksize[5], out_channels=ksize[6], kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 'Mid-level fetures'\n",
    "        self.conv7 = nn.Conv2d(in_channels=ksize[6], out_channels=ksize[7], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=ksize[7], out_channels=ksize[8], kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 'Colorization network'\n",
    "        self.conv9 = nn.Conv2d(in_channels=ksize[8], out_channels=ksize[9], kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Here comes upsample #1\n",
    "        \n",
    "        self.conv10 = nn.Conv2d(in_channels=ksize[9], out_channels=ksize[10], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv11 = nn.Conv2d(in_channels=ksize[10],out_channels=ksize[11], kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Here comes upsample #2        \n",
    "        \n",
    "        self.conv12 = nn.Conv2d(in_channels=ksize[11], out_channels=ksize[12], kernel_size=3, stride=1, padding=1)\n",
    "        self.conv13 = nn.Conv2d(in_channels=ksize[12], out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Low level\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = F.relu(self.conv4(out))\n",
    "        out = F.relu(self.conv5(out))\n",
    "        out = F.relu(self.conv6(out))\n",
    "        \n",
    "        # y = out\n",
    "        # z = out\n",
    "        # y → mid level\n",
    "        # z → global features \n",
    "        \n",
    "        # Mid level\n",
    "    \n",
    "        out = F.relu(self.conv7(out))\n",
    "        out = F.relu(self.conv8(out))\n",
    "        \n",
    "        # assert out.shape[1:] == (256, 28, 28), \"おわり： mid level\"\n",
    "        \n",
    "\n",
    "        # Colorization Net\n",
    "        out = F.relu(self.conv9(out))\n",
    "        \n",
    "        # assert out.shape[1:] == (128, 28, 28), \"おわり： conv9\"\n",
    "        \n",
    "        out = nn.functional.interpolate(input=out, scale_factor=2, mode='nearest')\n",
    "\n",
    "        # assert out.shape[1:] == (128, 56, 56), \"おわり： upsample1\"\n",
    "    \n",
    "        out = F.relu(self.conv10(out))\n",
    "        out = F.relu(self.conv11(out))\n",
    "        \n",
    "        out = nn.functional.interpolate(input=out, scale_factor=2, mode='nearest')\n",
    "\n",
    "\n",
    "        out = F.relu(self.conv12(out))\n",
    "        out = torch.sigmoid(self.conv13(out))\n",
    "        \n",
    "        out = nn.functional.interpolate(input=out, scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # assert out.shape[1:] == (2, 224, 224)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "Unnormalize and return RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_out2rgb(L, ab_out):\n",
    "    \"\"\"\n",
    "    L - original `L` channel\n",
    "    ab_out - learned `ab` channels which were the net's output\n",
    "    \n",
    "    Retruns: 3 channel RGB image\n",
    "    \"\"\"\n",
    "    # Convert to numpy and unnnormalize\n",
    "    L = L.numpy() * 100.0\n",
    "    \n",
    "    ab_out = np.floor(ab_out.numpy() * 255.0) - 128.0 \n",
    "    \n",
    "    # Transpose axis to HxWxC again\n",
    "    L = L.transpose((1, 2, 0))\n",
    "    ab_out = ab_out.transpose((1, 2, 0))\n",
    "\n",
    "    # Stack layers\n",
    "    lab_stack = np.dstack((L, ab_out))\n",
    "    \n",
    "    return color.lab2rgb(lab_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = ColNet()\n",
    "mse = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "model_filename = \"\"\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    dev_loss = 0.0\n",
    "    \n",
    "    ################################ \n",
    "    ##   TRAINING   ################\n",
    "    ################################\n",
    "\n",
    "    print(\"Epoch {} / {}\".format(epoch + 1, EPOCHS))\n",
    "    \n",
    "    # Turn train mode on\n",
    "    net.train() \n",
    "    for batch_idx, train_data in enumerate(trainloader):\n",
    "\n",
    "        L, ab, _ = train_data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        ab_out = net(L)\n",
    "        \n",
    "        \n",
    "        assert ab.shape == ab_out.shape\n",
    "        \n",
    "        loss = mse(ab, ab_out)      \n",
    "        \n",
    "#         myloss_avg = ((ab - ab_out)**2).mean().item()\n",
    "#         myloss_sum = ((ab - ab_out)**2).sum().item()\n",
    "#         print(\"loss: {:.4f}  \\t myloss_avg: {:.4f} \\t myloss_sum {:.4f}\"\n",
    "#               .format(loss, myloss_avg, myloss_sum))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "       \n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        \n",
    "        print('[{} / {}] batch loss: {:.3f}'\n",
    "            .format(batch_idx + 1, len(trainloader), batch_loss))\n",
    "        epoch_loss += batch_loss\n",
    "    \n",
    "    #\n",
    "    # len(trainloader) - ilość batch\n",
    "    # len(trainloader.dataset) - ilość zdjęć\n",
    "    # Uwaga: ilość_batch * batch_size != ilość zdjęć \n",
    "    # (bo ostatni batch może mieć mniej zdjęć, \n",
    "    # gdy batch_size nie dzieli #zdj)\n",
    "    # \n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print('SUM of all batch losses in epoch: {:.2f}'.format(epoch_loss))\n",
    "    print('MEAN over all batches in epoch: {:.2f}'.format(epoch_loss/len(trainloader)))\n",
    "    \n",
    "    epoch_loss /= len(trainloader)\n",
    "    \n",
    "    #################################\n",
    "    ##  VALIDATE on dev set  ########\n",
    "    #################################\n",
    "    print(\"\\nValidating...\")\n",
    "    \n",
    "    # Turn eval mode on\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        dev_loss = 0.0\n",
    "        \n",
    "        for batch_idx, dev_data in enumerate(devloader):\n",
    "\n",
    "            L_dev, ab_dev, _ = dev_data\n",
    "\n",
    "            ab_dev_output = net(L_dev)\n",
    "\n",
    "            assert ab_dev.shape == ab_dev_output.shape\n",
    "            \n",
    "            dev_batch_loss = mse(ab_dev_output, ab_dev)\n",
    "            \n",
    "#             mydev_loss_sum = ((ab_dev - ab_dev_output)**2).sum().item()\n",
    "#             mydev_loss_avg = ((ab_dev - ab_dev_output)**2).mean().item()            \n",
    "#             print(\"dev_loss {} \\t  myloss_avg {} \\t myloss_sum {}\"\n",
    "#                   .format(dev_loss, mydev_loss_avg, mydev_loss_sum))\n",
    "\n",
    "            dev_loss += dev_batch_loss\n",
    "            print(\"[{} / {}] dev batch loss: {}\"\n",
    "                  .format(batch_idx+1, len(devloader), dev_batch_loss))\n",
    "            \n",
    "            \n",
    "            \n",
    "        print(\"sum of dev losses {}\".format(dev_loss.item()))\n",
    "        print(\"mean of dev losses {}\".format(dev_loss.item()/len(devloader)))\n",
    "        \n",
    "        \n",
    "        loss_history.append((epoch_loss, dev_loss/len(devloader)))\n",
    "\n",
    "            \n",
    "    # Save\n",
    "    # model_filename = './model/colnet{}.pt'.format(time.strftime(\"%y%m%d-%H-%M-%S\"))\n",
    "    model_filename = './model/colnet.pt'\n",
    "    torch.save(net.state_dict(), model_filename)\n",
    "    print('\\nsaved model to {}'.format(model_filename))\n",
    "    \n",
    "    print('~ end of epoch {}\\n\\n\\n'.format(epoch + 1))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_epoch = [h[0] for h in loss_history]\n",
    "hist_dev   = [h[1] for h in loss_history]\n",
    "Xs = np.arange(0, EPOCHS, 1)\n",
    "plt.plot(Xs, hist_epoch, label='Epoch loss')\n",
    "plt.plot(Xs, hist_dev, label='Dev loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='upper right');\n",
    "# plt.ylim(1500, 2200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf out/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_filename = \"./model/colnet181105-21-36-20.pt\"\n",
    "model_filename = \"./model/colnet.pt\"\n",
    "\n",
    "print(\"Make sure you're using up to date model!!!\")    \n",
    "print(\"Colorizing {} using {}\\n\".format(img_dir_test, model_filename))\n",
    "\n",
    "\n",
    "net = ColNet()\n",
    "net.load_state_dict(torch.load(model_filename))\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_no, data in enumerate(testloader):\n",
    "        \n",
    "        print(\"Processing batch {} / {}\".format(batch_no + 1, len(testloader)))\n",
    "        L, ab, name = data\n",
    "        ab_outputs = net(L)\n",
    "        \n",
    "        for i in range(L.shape[0]):\n",
    "            img = net_out2rgb(L[i], ab_outputs[i])\n",
    "            io.imsave(os.path.join(\"./out/\", name[i]), img)\n",
    "\n",
    "print(\"Saved all photos to ./out/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
